{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:08:25.627729Z",
     "start_time": "2020-12-24T15:08:22.729470Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hazm import *\n",
    "from itertools import groupby\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azin's Work (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:09:36.639246Z",
     "start_time": "2020-12-24T15:09:36.622686Z"
    }
   },
   "outputs": [],
   "source": [
    "answers_clean = pd.read_csv('answers_clean.csv', index_col=0)\n",
    "# I'm not really sure about removing stop words\n",
    "text_file = open(\"stop_words_short.txt\", encoding=\"utf8\")\n",
    "stop_words = text_file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:27:59.502817Z",
     "start_time": "2020-12-24T15:27:59.487008Z"
    }
   },
   "outputs": [],
   "source": [
    "answers_clean = pd.read_csv(\"mh_clean.csv\", index_col=0)\n",
    "with open(\"stop_words_short.txt\", encoding=\"utf8\") as text_file:\n",
    "    stop_words = text_file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:17.745408Z",
     "start_time": "2020-12-24T15:12:17.738010Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_sw(sent_tokens):\n",
    "    final_tokens = []\n",
    "    for token in sent_tokens:\n",
    "        if token not in stop_words:\n",
    "            final_tokens.append(token)\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:17.987018Z",
     "start_time": "2020-12-24T15:12:17.980091Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_tokens(tokens):\n",
    "    unique_list = []\n",
    "    for x in tokens:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:13:52.871938Z",
     "start_time": "2020-12-24T15:13:52.862747Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_tokenizing(remove_stop_word=True):\n",
    "    sents = answers_clean['sentences']\n",
    "    all_tokens = []\n",
    "    max_tokens_per_line = 0\n",
    "    for sent in sents:\n",
    "        tokens = word_tokenize(sent)\n",
    "        if remove_stop_word:\n",
    "            tokens = remove_sw(tokens)\n",
    "        if len(tokens) > max_tokens_per_line:\n",
    "            max_tokens_per_line = len(tokens)\n",
    "        all_tokens.append(tokens)\n",
    "    return all_tokens, max_tokens_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:19.541206Z",
     "start_time": "2020-12-24T15:12:19.532097Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_df(all_tokens):\n",
    "    DF = {}\n",
    "    for tokens in all_tokens:\n",
    "        tokens = unique_tokens(tokens)\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                DF[token] += 1\n",
    "            except:\n",
    "                DF[token] = 1\n",
    "\n",
    "    return DF, len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:20.171153Z",
     "start_time": "2020-12-24T15:12:20.162477Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tf_idf(all_tokens, DF, number_of_documents):\n",
    "    sentenses = []\n",
    "    for sent_tokens in all_tokens:\n",
    "        sentense = []\n",
    "        number_of_tokens = len(sent_tokens)\n",
    "        tf = dict(zip([key for key, group in groupby(sent_tokens)],\n",
    "                      [len(list(group)) for key, group in groupby(sent_tokens)]))\n",
    "        for token in sent_tokens:\n",
    "            idf = math.log(number_of_documents/DF[token])\n",
    "            word_tf = tf[token] / number_of_tokens\n",
    "            sentense.append(word_tf * idf)\n",
    "        sentenses.append(sentense)\n",
    "\n",
    "    return sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:20.723401Z",
     "start_time": "2020-12-24T15:12:20.715827Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_padding(vectors, max_tokens_per_line):\n",
    "    padded_sentenses = []\n",
    "    for vector in vectors:\n",
    "        if len(vector) < max_tokens_per_line:\n",
    "            pad = max_tokens_per_line - len(vector)\n",
    "            vector.extend([0] * pad)\n",
    "            padded_sentenses.append(vector)\n",
    "        else:\n",
    "            padded_sentenses.append(vector)\n",
    "    return padded_sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:12:39.903945Z",
     "start_time": "2020-12-24T15:12:39.891412Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    all_tokens, max_tokens_per_line = word_tokenizing()\n",
    "    DF, number_of_documents = calculate_df(all_tokens)\n",
    "    vectors = calculate_tf_idf(all_tokens, DF, number_of_documents)\n",
    "    assert(len(vectors) == len(all_tokens))\n",
    "    padded = max_padding(vectors, max_tokens_per_line)\n",
    "    df = pd.DataFrame(padded)\n",
    "    df.to_csv(\"mh_tf_idf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T15:13:57.813785Z",
     "start_time": "2020-12-24T15:13:57.350267Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
